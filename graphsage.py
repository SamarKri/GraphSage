# -*- coding: utf-8 -*-
"""GraphSage.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bg-F-N27qJ9vpbUwcBTG598-JOcPCZAx

# **PROJET GRAPHSAGE**

---


> D'après l'article **Inductive Representation Learning on Large Graphs** [Hamilton et al., 2017](https://arxiv.org/abs/1706.02216)

## **Samar KRIMI - M2 BDIA - 2024-2025**
"""

# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

# Libérer la mémoire GPU
import torch
torch.cuda.empty_cache()

"""# Installation des Bibliothèques"""

!pip install torch torchvision torchaudio torch-geometric networkx transformers seaborn

"""# Importations"""

# Importations complètes
import torch
import warnings
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.loader import DataLoader
from torch_geometric.datasets import Planetoid, PPI
from torch_geometric.nn import SAGEConv, GATConv, GCNConv
from torch_geometric.utils import negative_sampling
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (f1_score, accuracy_score, confusion_matrix,
                            classification_report, precision_score, recall_score)
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from copy import deepcopy
import requests
import networkx as nx
from torch_geometric.data import Data
from torch_geometric.utils import from_networkx
from sklearn.feature_extraction.text import TfidfVectorizer
import random
from sklearn.model_selection import cross_val_predict
from sklearn.decomposition import TruncatedSVD
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.multioutput import MultiOutputClassifier
from collections import defaultdict
import time

"""# Configurations"""

# Détection automatique du périphérique : GPU si disponible, sinon CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# Application d'un style graphique à matplotlib pour des graphiques plus esthétiques
plt.style.use('ggplot')

"""# Dataloader

### Load OpenAlex
"""

def load_openalex_dataset(min_nodes=1000, max_nodes=1500):
    """
    Charge un dataset à partir de l'API OpenAlex.

    Les étapes sont les suivantes :
      1. Requête sur l'API pour récupérer les travaux principaux en fonction de plusieurs requêtes thématiques.
      2. Construction d'un graphe NetworkX avec les travaux principaux et leurs références.
      3. Vectorisation des titres des travaux principaux pour créer des features.
      4. Conversion du graphe en objet PyG Data avec les features et les étiquettes.
      5. Ajout de masques de séparation pour l'entraînement, validation et test.
      6. Si le nombre de nœuds est insuffisant, un graphe de secours est fusionné.
    """

    queries = [
        "climate change",
        "biodiversity",
        "renewable energy",
        "sustainability",
        "carbon footprint",
        "environmental policy",
        "ecological transition",
        "green technology"
    ]

    # En-têtes HTTP pour simuler un navigateur (utile pour éviter certains blocages)
    headers = {'User-Agent': 'Mozilla/5.0'}

    try:
        # === 1. Récupération des travaux principaux ===
        main_works = []

        # Prépare l'URL de l'API avec la recherche dans le titre et 200 résultats par page
        for query in queries:
            url = f"https://api.openalex.org/works?filter=title.search:{query}&per-page=200"

            # Récupération sur plusieurs pages
            for page in range(1, 4):  # 3 pages par requête
                try:
                    time.sleep(0.5)  # Respect du rate limiting de l'API

                    # Effectue la requête HTTP
                    response = requests.get(f"{url}&page={page}", headers=headers)
                    response.raise_for_status()

                     # Extrait les résultats (travaux) de la réponse JSON
                    works = response.json().get("results", [])
                    main_works.extend(works)

                    # Arrêt si on a assez de nœuds
                    if len(main_works) >= max_nodes * 0.7:  # 70% de nœuds principaux
                        break
                except Exception as e:

                    # En cas d'erreur sur la page, affiche l'erreur et continue avec la page suivante
                    print(f"Erreur page {page} pour '{query}': {str(e)}")
                    continue

        # === Échantillonnage si trop de résultats ===
        # Si le nombre de travaux principaux dépasse 80% de max_nodes, on en choisit un sous-échantillon aléatoire
        if len(main_works) > max_nodes * 0.8:
            main_works = random.sample(main_works, int(max_nodes * 0.8))

        # Vérifie qu'on a bien récupéré des travaux
        if not main_works:
            raise Exception("Aucun résultat trouvé")

         # === 2. Construction du graphe avec NetworkX ===
        G = nx.Graph()
        print(f"Found {len(main_works)} main works")

        # Ajout des travaux principaux avec métadonnées
        for work in main_works:
            title = work.get("title", "").strip()
            if title:
                G.add_node(work["id"],
                         title=title,
                         year=work.get("publication_year"),
                         citations=work.get("cited_by_count", 0),
                         type='main')

        # === 3. Ajout des références (avec parallélisation partielle) ===
        reference_count = 0
        max_references = max_nodes - len(main_works)

        for i, work in enumerate(main_works[:int(len(main_works)*0.8)]):  # 80% des travaux principaux

            # Références directes
            for ref in work.get("referenced_works", [])[:25]:  # 25 réf max par travail
                if ref not in G and reference_count < max_references:
                    G.add_node(ref, title="", type='ref1')
                    reference_count += 1
                    G.add_edge(work["id"], ref)

                # Arrêt si on atteint la limite
                if reference_count >= max_references:
                    break
            # Affiche un message de progression toutes les 50 itérations
            if i % 50 == 0:
                print(f"Processed {i}/{len(main_works)} works, {reference_count} references added")

            if reference_count >= max_references:
                break

        print(f"Graph constructed - Nodes: {len(G.nodes())}, Edges: {len(G.edges())}")

        # === 4. Vectorisation des titres et création des features ===
        # Construit un corpus à partir des titres des travaux principaux
        corpus = [G.nodes[node]['title'] for node in G.nodes() if G.nodes[node]['type'] == 'main']
        # Initialise le vectoriseur TF-IDF pour obtenir jusqu'à 256 features, en excluant les stop-words anglais et en considérant des n-grammes de 1 à 2
        vectorizer = TfidfVectorizer(max_features=256, stop_words='english', ngram_range=(1,2))
        # Applique le vectoriseur sur le corpus et transforme en tableau numpy
        X_tfidf = vectorizer.fit_transform(corpus).toarray()

        # Features pour tous les nœuds
        features = np.zeros((len(G.nodes()), 256))
        node_list = list(G.nodes())
        # Identifie les indices des nœuds principaux dans la liste
        main_node_indices = [i for i, node in enumerate(node_list) if G.nodes[node]['type'] == 'main']

        # Remplissage pour les nœuds principaux
        for idx, main_idx in enumerate(main_node_indices):
            features[main_idx] = X_tfidf[idx]

        # Remplissage pour les références (avec une meilleure initialisation)
        ref_indices = [i for i, node in enumerate(node_list) if G.nodes[node]['type'] != 'main']
        features[ref_indices] = np.random.normal(loc=0.1, scale=0.05, size=(len(ref_indices), 256))

        # === 5. Conversion du graphe en objet PyG Data ===
        data = from_networkx(G)
        # Conversion des features en tenseur PyTorch
        data.x = torch.tensor(features, dtype=torch.float)

        # Étiquettes multi-classes (0: main, 1: ref1, 2: ref2)
        data.y = torch.tensor([0 if G.nodes[node]['type'] == 'main' else
                             1 if G.nodes[node]['type'] == 'ref1' else 2
                             for node in G.nodes()], dtype=torch.long)

        # Ajout des masques avec meilleure répartition train/val/test
        data = add_masks(data, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)

        # === 6. Vérification et ajustement final du graphe ===
        # Si le graphe contient moins de nœuds que min_nodes, on le complète avec un graphe de secours
        if len(G.nodes()) < min_nodes:
            print(f"Warning: Only {len(G.nodes())} nodes, augmenting with fallback")
            fallback_data = create_fallback_graph(min_nodes - len(G.nodes()))
            data = merge_graphs(data, fallback_data)

        print(f"Final graph - Nodes: {data.num_nodes}, Edges: {data.num_edges}, "
              f"Features: {data.num_features}, Classes: {len(torch.unique(data.y))}")
        return data.to(device)

    except Exception as e:
        print(f"Error loading OpenAlex: {str(e)}")
        return create_fallback_graph(min_nodes).to(device)

def merge_graphs(data1, data2):
    """
    Fusionne deux graphes PyG en concaténant les features, les étiquettes, les arêtes et les masques.
    """
    x = torch.cat([data1.x, data2.x], dim=0)
    y = torch.cat([data1.y, data2.y], dim=0)

    # Ajustement des indices des arêtes du deuxième graphe
    edge_index2 = data2.edge_index + data1.num_nodes
    edge_index = torch.cat([data1.edge_index, edge_index2], dim=1)

    # Fusion des masques (train, validation, test)
    merged_data = Data(x=x, edge_index=edge_index, y=y)
    merged_data.train_mask = torch.cat([data1.train_mask, data2.train_mask])
    merged_data.val_mask = torch.cat([data1.val_mask, data2.val_mask])
    merged_data.test_mask = torch.cat([data1.test_mask, data2.test_mask])

    return merged_data

def get_references(work_id):
    """Fonction simplifiée pour récupérer les références d'un travail"""
    # Implémentation basique - à adapter avec une vraie requête API si nécessaire
    return [f"R{work_id[-5:]}{i}" for i in range(8)]

def create_fallback_graph(size=800):
    """Graphe de secours si l'API échoue"""
    G = nx.barabasi_albert_graph(size, 3)  # Graphe synthétique plus dense
    data = from_networkx(G)
    data.x = torch.rand((size, 128))
    data.y = torch.randint(0, 2, (size,))
    data = add_masks(data)
    return data

def process_references(work_id, G, depth=0):
    """Traitement parallèle des références"""
    if depth >= 2 or len(G.nodes) > 3000:
        return

    try:
      # Récupère un maximum de 15 références
        refs = fetch_references(work_id)[:15]
        for ref in refs:
            if ref not in G:
                # Attribue un type en fonction de la profondeur
                G.add_node(ref, type=f'ref_{depth+1}')
            G.add_edge(work_id, ref)

            if depth < 1:  # Limite la profondeur
                process_references(ref, G, depth+1)
    except:
        pass

def fetch_paginated(url, pages=3, per_page=200):
    """Récupération paginée avec cache"""
    results = []
    for page in range(1, pages+1):
        try:
            response = requests.get(f"{url}&per-page={per_page}&page={page}",
                                  headers={'User-Agent': 'Mozilla/5.0'})
            results.extend(response.json().get('results', []))
            time.sleep(0.5)
        except:
            continue
    return results

def fetch_references(work_id):
    """Fonction Mock pour récupérer les références - à remplacer par des vraies requêtes"""
    return [f"R{work_id[-7:]}{i}" for i in range(15)]

"""### Load All datasets"""

def load_all_datasets():
    datasets = []

    # Citeseer
    try:
        citeseer = Planetoid(root="/tmp/Citeseer", name="Citeseer")
        data = citeseer[0]
        data = add_masks(data)
        datasets.append({
            'name': 'Citeseer',
            'source': 'Citeseer',  # Ajout de l'attribut source
            'data': data.to(device),
            'is_multilabel': False,
            'classes': citeseer.num_classes
        })
    except Exception as e:
        print(f"Erreur chargement Citeseer: {e}")

    # OpenAlex
    try:
        openalex_data = load_openalex_dataset()
        datasets.append({
            'name': 'OpenAlex',
            'source': 'OpenAlex',  # Ajout de l'attribut source
            'data': openalex_data.to(device),
            'is_multilabel': False,
            'classes': len(torch.unique(openalex_data.y))
        })
    except Exception as e:
        print(f"Erreur chargement OpenAlex: {e}")

    # PPI
    try:
        ppi_train = PPI(root="/tmp/PPI", split='train')[0]
        ppi_val = PPI(root="/tmp/PPI", split='val')[0]
        ppi_test = PPI(root="/tmp/PPI", split='test')[0]

        datasets.append({
            'name': 'PPI',
            'source': 'PPI',  # Ajout de l'attribut source
            'train_data': ppi_train.to(device),
            'val_data': ppi_val.to(device),
            'test_data': ppi_test.to(device),
            'is_multilabel': True,
            'separate_splits': True
        })
    except Exception as e:
        print(f"Erreur chargement PPI: {e}")

    return datasets

"""# Models"""

def deepwalk1(graph, labels=None, walk_length=30, num_walks=20,
             window_size=5, embedding_dim=256, alpha=0.0, dataset_name=None):
    """
    Implémente l'algorithme DeepWalk (sans utiliser gensim) pour générer des embeddings à partir d'un graphe.

    Paramètres :
      - graph : un objet NetworkX représentant le graphe.
      - labels : (optionnel) dictionnaire ou tableau de labels pour chaque nœud, utilisé pour favoriser
                 des marches (walks) avec des nœuds de même label si alpha > 0.
      - walk_length : longueur d'une marche aléatoire.
      - num_walks : nombre de marches générées par nœud.
      - window_size : taille de la fenêtre pour le calcul de la matrice de co-occurrence.
      - embedding_dim : dimension des embeddings résultants.
      - alpha : probabilité de privilégier des voisins ayant le même label.
      - dataset_name : nom du dataset (ex: "ppi") pour adapter le test de similarité de labels.

    Processus :
      1. Génère plusieurs marches aléatoires sur le graphe.
      2. Pour chaque marche, si labels est fourni et alpha > 0, privilégie les voisins avec le même label.
      3. Construit une matrice de co-occurrence basée sur les nœuds visités dans une fenêtre donnée.
      4. Applique une décomposition en valeurs singulières (SVD) sur la matrice pour obtenir les embeddings.

    Retour :
      Un dictionnaire associant chaque nœud à son embedding (tableau de dimension embedding_dim).
    """
    # --- Génération des marches aléatoires ---
    walks = []
    nodes = list(graph.nodes())

    for _ in range(num_walks):
        random.shuffle(nodes)
        for node in nodes:
            walk = [node]
            current_node = node
            for _ in range(walk_length - 1):
                neighbors = list(graph.neighbors(current_node))
                if not neighbors:
                    break
                if labels is not None and alpha > 0:
                    # Pour le dataset "ppi", le test est fait via .all() (si les labels sont multi-dimensionnels)
                    same_label_neighbors = [
                        n for n in neighbors
                        if (labels[n] == labels[current_node]).all()
                    ] if dataset_name == "ppi" else [
                        n for n in neighbors
                        if labels[n] == labels[current_node]
                    ]
                    # Avec une probabilité alpha, choisit un voisin ayant le même label si disponible
                    if same_label_neighbors and random.random() < alpha:
                        current_node = random.choice(same_label_neighbors)
                    else:
                        current_node = random.choice(neighbors)
                else:
                    # Sinon, choisit simplement un voisin aléatoirement
                    current_node = random.choice(neighbors)
                walk.append(current_node)
            walks.append(walk)

    # # --- Construction de la matrice de co-occurrence avec tous les nœuds ---
    all_nodes = sorted(graph.nodes())
    node_to_idx = {node: idx for idx, node in enumerate(all_nodes)}
    matrix = np.zeros((len(all_nodes), len(all_nodes)))

    for walk in walks:
        for i in range(len(walk)):
            for j in range(max(0, i-window_size), min(len(walk), i+window_size+1)):
                if i != j:
                    matrix[node_to_idx[walk[i]], node_to_idx[walk[j]]] += 1

    # --- Réduction de dimension par SVD ---
    svd = TruncatedSVD(n_components=embedding_dim, random_state=42)
    embeddings = svd.fit_transform(matrix)
    # Retourne un dictionnaire associant chaque nœud à son embedding
    return {node: embeddings[node_to_idx[node]] for node in all_nodes}

#########################################
# Classes pour DeepWalk et agrégateurs  #
#########################################

class UnsupervisedDeepWalk(nn.Module):
    """
    Classe pour DeepWalk en mode non supervisé.
    Génère les embeddings en convertissant d'abord le graphe PyG en NetworkX puis en appelant deepwalk1.
    """
    def __init__(self, embedding_dim=128):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.embeddings = None

    def forward(self, data):
        # Conversion du graphe PyG en NetworkX
        G = nx.Graph()

        # Ajout de tous les nœuds, même isolés
        for i in range(data.num_nodes):
            G.add_node(i)

        # Ajout des arêtes
        edge_list = data.edge_index.t().cpu().numpy()
        G.add_edges_from(edge_list)

        # Affichage d'un message et génération des embeddings via deepwalk1
        print("Génération des embeddings DeepWalk...")
        self.embeddings = deepwalk1(
            G,
            walk_length=40,
            num_walks=20,
            window_size=10,
            embedding_dim=self.embedding_dim
        )

        # Vérification que les embeddings ont été générés
        if self.embeddings is None:
            raise ValueError("Les embeddings DeepWalk n'ont pas été générés!")

        print(f"Embeddings générés pour {len(self.embeddings)} nœuds")

        # Conversion en tensor PyTorch normalisé
        node_ids = sorted(self.embeddings.keys())
        emb_tensor = torch.stack([
            torch.tensor(self.embeddings[n_id], dtype=torch.float32)
            for n_id in node_ids
        ]).to(device)

        return F.normalize(emb_tensor, p=2, dim=1)

    def encode(self, data):
        # Méthode d'encodage identique à forward
        return self.forward(data)

class SupervisedDeepWalk(nn.Module):
    """
    Variante supervisée de DeepWalk.
    En plus de générer les embeddings, un classifieur linéaire est appliqué pour prédire les labels.
    """
    def __init__(self, embedding_dim=256, out_size=10):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.out_size = out_size
        self.embeddings = None
        self.classifier = nn.Linear(embedding_dim, out_size)

    def forward(self, data):
        # Conversion du graphe PyG en NetworkX
        G = nx.Graph()

        # Ajouter tous les nœuds du graphe
        for i in range(data.num_nodes):
            G.add_node(i)

        # Ajouter les arêtes
        edge_list = data.edge_index.t().cpu().numpy()
        G.add_edges_from(edge_list)

        # Ajouter les attributs de nœud
        labels = data.y.cpu().numpy()
        is_multilabel = len(labels.shape) > 1

        # Génère les embeddings supervisés
        self.embeddings = deepwalk1(
            G,
            labels=labels,
            alpha=0.7,
            dataset_name="ppi" if is_multilabel else "citeseer",
            walk_length=60 if is_multilabel else 20,
            num_walks=40 if is_multilabel else 10,
            window_size=5,
            embedding_dim=self.embedding_dim
        )

        # Convertir en tensor et classifier
        node_ids = sorted(self.embeddings.keys())
        emb_tensor = torch.stack([
            torch.tensor(self.embeddings[n_id], dtype=torch.float32)
            for n_id in node_ids
        ]).to(device)
        # Normalise les embeddings et renvoie la sortie du classifieur
        return self.classifier(F.normalize(emb_tensor, p=2, dim=1))

class StableLSTMAggregator(nn.Module):
    """
    Agrégateur basé sur une LSTM bidirectionnelle stable.
    Utilisé pour agréger les features des voisins d'un nœud.
    """
    def __init__(self, in_size, out_size):
        super().__init__()

        # LSTM bidirectionnelle
        self.lstm = nn.LSTM(in_size, out_size // 2, bidirectional=True, batch_first=True)
        self.out_proj = nn.Linear(out_size, out_size)

    def forward(self, neighbors):
        """
        Agrège les features des voisins.

        Paramètre :
          - neighbors : tenseur de forme [num_neighbors, feature_size]

        Retourne :
          Un vecteur agrégé de dimension out_size.
        """
        if len(neighbors) == 0:
            return torch.zeros(self.out_proj.out_features, device=neighbors.device)

        # Ajout d'une dimension batch
        neighbors = neighbors.unsqueeze(0)  # [1, num_neighbors, feature_size]

        # Initialisation des états cachés
        h0 = torch.zeros(2, 1, self.lstm.hidden_size, device=neighbors.device)
        c0 = torch.zeros(2, 1, self.lstm.hidden_size, device=neighbors.device)

        # Passage dans le LSTM
        out, _ = self.lstm(neighbors, (h0, c0))

        # Agrégation par moyenne
        aggregated = out.mean(dim=1)
        return self.out_proj(aggregated.squeeze(0))

class SupervisedGNN(nn.Module):
    """
    Classe de base pour un modèle GNN supervisé.
    Combine un encodeur (pour générer des embeddings) et un classifieur linéaire.
    """
    def __init__(self, encoder, hidden_size, out_size):
        super().__init__()
        self.encoder = encoder
        self.classifier = nn.Linear(hidden_size, out_size)

    def forward(self, x, edge_index):
        h = self.encoder(x, edge_index)
        return self.classifier(h)

class GraphSAGEEncoder(nn.Module):
    """
    Encodeur GraphSAGE permettant d'agréger les informations de voisinage.
    Peut utiliser différents agrégateurs, y compris une agrégation par LSTM.
    """
    def __init__(self, in_size, hidden_size, aggregator='mean'):
        super().__init__()
        self.aggregator = aggregator
        if aggregator == 'lstm':
            # Utilise l'agrégateur LSTM stable pour combiner les voisins
            self.agg_module = StableLSTMAggregator(hidden_size, hidden_size)
            self.conv1 = SAGEConv(in_size, hidden_size, aggr='mean')
        else:
            self.conv1 = SAGEConv(in_size, hidden_size, aggr=aggregator)
        self.conv2 = SAGEConv(hidden_size, hidden_size, aggr=aggregator)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        if self.aggregator == 'lstm':
            # Création d'un dictionnaire pour regrouper les voisins de chaque nœud
            neighbor_dict = defaultdict(list)
            for src, dst in edge_index.t().tolist():
                neighbor_dict[dst].append(src)
            x_agg = torch.zeros_like(x)
            # Agrège les features pour chaque nœud à l'aide de la LSTM
            for node in range(x.size(0)):
                neighbors = x[neighbor_dict.get(node, [])]
                x_agg[node] = self.agg_module(neighbors) if len(neighbors) > 0 else x[node]
            x = x_agg
        return self.conv2(x, edge_index)

class GCNEncoder(nn.Module):
    """
    Encodeur basé sur le GCN (Graph Convolutional Network).
    Composé de deux couches GCN avec activation ReLU.
    """
    def __init__(self, in_size, hidden_size):
        super().__init__()
        self.conv1 = GCNConv(in_size, hidden_size)
        self.conv2 = GCNConv(hidden_size, hidden_size)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        return self.conv2(x, edge_index)

class UnsupervisedGNN(nn.Module):
    """
    Modèle GNN non supervisé.
    Utilise un encodeur pour générer des embeddings puis une tête de projection pour normaliser ces embeddings.
    """
    def __init__(self, encoder, hidden_size, emb_size):
        super().__init__()
        self.encoder = encoder
        self.proj_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, emb_size))

    def forward(self, x, edge_index):
        h = self.encoder(x, edge_index)
        z = self.proj_head(h)
        return F.normalize(z, p=2, dim=1)

    def encode(self, x, edge_index):
        return self.encoder(x, edge_index)


class LSTMAggregator(nn.Module):
    """
    Agrégateur LSTM utilisé notamment pour le dataset PPI.
    Agrège les features d'un nœud en combinant ses voisins via une LSTM bidirectionnelle.
    """
    def __init__(self, input_dim, output_dim):
        super(LSTMAggregator, self).__init__()
        assert output_dim % 2 == 0, "Output dimension must be even for bidirectional LSTM"
        self.lstm = nn.LSTM(input_dim, output_dim // 2, bidirectional=True, batch_first=True)

    def forward(self, x):
        x = x.unsqueeze(1)
        _, (h_n, _) = self.lstm(x)
        h_n = h_n.view(2, -1, h_n.size(-1))
        h_n = torch.cat([h_n[0], h_n[1]], dim=-1)
        return h_n

class GraphSAGE_LSTM(nn.Module):
    """
    Modèle GraphSAGE avec agrégation par LSTM, adapté pour le dataset PPI.

    Processus :
      - Plusieurs couches SAGEConv pour transformer les features.
      - Une agrégation LSTM est appliquée aux features intermédiaires.
      - Une dernière couche SAGEConv produit la sortie finale.
    """
    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):
        super(GraphSAGE_LSTM, self).__init__()
        self.convs = nn.ModuleList()

        # Première couche SAGEConv
        self.convs.append(SAGEConv(in_channels, hidden_channels, aggr='mean'))

        # Couches intermédiaires SAGEConv
        for _ in range(num_layers - 2):
            self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr='mean'))

        # Dernière couche SAGEConv
        self.convs.append(SAGEConv(hidden_channels, out_channels, aggr='mean'))

        # Aggrégation LSTM
        self.lstm_agg = LSTMAggregator(hidden_channels, hidden_channels)

    def forward(self, x, edge_index):
        # Application des couches SAGEConv intermédiaires avec ReLU et dropout
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, p=0.5, training=self.training)

        # Agrégation par LSTM
        x = self.lstm_agg(x)

        # Couche finale SAGEConv
        x = self.convs[-1](x, edge_index)
        return torch.sigmoid(x)

class GraphSAGE_Unsupervised(nn.Module):
    """
    Modèle GraphSAGE pour apprentissage non supervisé (adapté à PPI).
    Combine un encodeur GraphSAGE_LSTM et un décodeur pour reconstruire les features d'entrée.
    """
    def __init__(self, in_channels, hidden_channels, emb_channels):
        super(GraphSAGE_Unsupervised, self).__init__()
        self.encoder = GraphSAGE_LSTM(in_channels, hidden_channels, emb_channels, num_layers=2)
        self.decoder = nn.Sequential(
            nn.Linear(emb_channels, hidden_channels),
            nn.ReLU(),
            nn.Linear(hidden_channels, in_channels)
        )

    def forward(self, x, edge_index):
        z = self.encoder(x, edge_index)
        x_recon = self.decoder(z)
        return z, x_recon


def deepwalk(graph, labels=None, walk_length=30, num_walks=20,
             window_size=5, embedding_dim=256, alpha=0.0, dataset_name=None):
    """
    Variante de la fonction deepwalk1, adaptée pour le dataset PPI.

    Fonctionne de la même manière que deepwalk1 :
      - Génère des marches aléatoires sur le graphe.
      - Construit une matrice de co-occurrence basée sur une fenêtre donnée.
      - Applique une SVD pour réduire la dimension et obtenir les embeddings.

    Retourne :
      Un dictionnaire associant chaque nœud à son embedding.
    """
    # Génération des walks
    walks = []
    nodes = list(graph.nodes())

    for _ in range(num_walks):
        random.shuffle(nodes)
        for node in nodes:
            walk = [node]
            current_node = node
            for _ in range(walk_length - 1):
                neighbors = list(graph.neighbors(current_node))
                if not neighbors:
                    break
                if labels is not None and alpha > 0:
                    same_label_neighbors = [
                        n for n in neighbors
                        if (labels[n] == labels[current_node]).all()
                    ] if dataset_name == "ppi" else [
                        n for n in neighbors
                        if labels[n] == labels[current_node]
                    ]
                    if same_label_neighbors and random.random() < alpha:
                        current_node = random.choice(same_label_neighbors)
                    else:
                        current_node = random.choice(neighbors)
                else:
                    current_node = random.choice(neighbors)
                walk.append(current_node)
            walks.append(walk)

    # --- Construction de la matrice de co-occurrence avec tous les nœuds ---
    all_nodes = sorted(graph.nodes())
    node_to_idx = {node: idx for idx, node in enumerate(all_nodes)}
    matrix = np.zeros((len(all_nodes), len(all_nodes)))

    for walk in walks:
        for i in range(len(walk)):
            for j in range(max(0, i-window_size), min(len(walk), i+window_size+1)):
                if i != j:
                    matrix[node_to_idx[walk[i]], node_to_idx[walk[j]]] += 1

    # --- Réduction de dimension par SVD ---
    svd = TruncatedSVD(n_components=embedding_dim, random_state=42)
    embeddings = svd.fit_transform(matrix)
    return {node: embeddings[node_to_idx[node]] for node in all_nodes}

"""# Train Functions"""

def train_supervised(model, data, optimizer, criterion, is_multilabel=False, use_masks=True):
    """
    Entraîne le modèle en mode supervisé sur un batch de données.

    Paramètres :
      - model : le modèle à entraîner.
      - data : l'objet PyG Data contenant les features (x), l'edge_index, les labels (y) et éventuellement les masques d'entraînement.
      - optimizer : l'optimiseur pour mettre à jour les poids du modèle.
      - criterion : la fonction de perte utilisée (ex : CrossEntropyLoss ou BCEWithLogitsLoss).
      - is_multilabel : booléen indiquant si la tâche est multi-label (nécessite une conversion des labels en float).
      - use_masks : booléen indiquant si l'on doit utiliser le masque de train pour sélectionner les nœuds.

    Processus :
      1. Met le modèle en mode entraînement.
      2. Réinitialise les gradients.
      3. Calcule la sortie du modèle.
      4. Sélectionne et calcule la perte sur les nœuds d'entraînement (ou sur l'ensemble si use_masks est False).
      5. Effectue la rétropropagation et la mise à jour des poids.
     """
    model.train()
    optimizer.zero_grad()
    out = model(data.x, data.edge_index)

    if use_masks:
      # Utilise le masque d'entraînement pour ne considérer que les nœuds d'entraînement
        if is_multilabel:
            # Conversion des labels en float pour une tâche multi-label
            loss = criterion(out[data.train_mask], data.y[data.train_mask].float())
        else:
            loss = criterion(out[data.train_mask], data.y[data.train_mask])
    else:
        # Calcule la perte sur tous les nœuds
        if is_multilabel:
            loss = criterion(out, data.y.float())
        else:
            loss = criterion(out, data.y)

    loss.backward()
    optimizer.step()
    return loss.item()

def train_contrastive(model, data, optimizer, temperature=0.5):
    """
    Entraîne le modèle en mode contrastif (non supervisé) sur un batch de données.

    Paramètres :
      - model : le modèle à entraîner.
      - data : l'objet PyG Data (doit contenir au moins des nœuds et des arêtes).
      - optimizer : l'optimiseur pour la mise à jour du modèle.
      - temperature : température utilisée dans la normalisation des similarités.

    Processus :
      1. Met le modèle en mode entraînement et réinitialise les gradients.
      2. Applique des perturbations robustes aux arêtes et aux features (dropout et masquage).
      3. Calcule deux représentations (z1 et z2) du graphe avec les deux vues perturbées.
      4. Corrige la taille des tenseurs si nécessaire.
      5. Calcule la perte contrastive entre les deux représentations.
      6. Effectue la rétropropagation et la mise à jour.
    """
    model.train()
    optimizer.zero_grad()

    # Vérifie que le graphe possède au moins une arête et un nœud
    assert data.edge_index.size(1) > 0, "Le graphe doit contenir des arêtes"
    assert data.x.size(0) > 0, "Le graphe doit contenir des nœuds"
    # Applique un edge dropout robuste
    edge_index1 = robust_edge_dropout(data.edge_index, data.num_nodes, p=0.2)
    # Applique un masquage sur les features
    edge_index2 = robust_feature_mask(data.x, data.edge_index, data.num_nodes, p=0.1)

    # Calcule les représentations (embeddings)
    z1 = model(data.x, edge_index1)
    z2 = model(data.x, edge_index2)

    # Ajuste les dimensions
    if z1.size(0) != z2.size(0):
        min_size = min(z1.size(0), z2.size(0))
        z1 = z1[:min_size]
        z2 = z2[:min_size]
    # Calcul de la perte contrastive
    loss = contrastive_loss(z1, z2, temperature)
    loss.backward()
    optimizer.step()
    return loss.item()

def robust_edge_dropout(edge_index, num_nodes, p=0.2):
    """
    Applique un dropout sur les arêtes de manière robuste.

    Paramètres :
      - edge_index : tensor des arêtes.
      - num_nodes : nombre total de nœuds.
      - p : taux de dropout (proportion d'arêtes à supprimer).

    Retourne :
      - Un nouveau edge_index avec un certain pourcentage d'arêtes supprimées.

    Remarque : On conserve au moins 10% des arêtes pour garantir une structure minimale.
    """
    if p == 0 or edge_index.size(1) == 0:
        return edge_index

    # Calcul du nombre minimal d'arêtes
    min_edges = max(1, int(0.1 * edge_index.size(1)))
    keep = max(min_edges, int((1 - p) * edge_index.size(1)))

    perm = torch.randperm(edge_index.size(1))
    edge_index = edge_index[:, perm[:keep]]

    return edge_index

def robust_feature_mask(x, edge_index, num_nodes, p=0.1):
    """
    Applique un masquage sur les features de manière robuste.

    Paramètres :
      - x : matrice des features.
      - edge_index : tensor des arêtes (non modifié ici).
      - num_nodes : nombre total de nœuds.
      - p : taux de masquage (proportion de features à masquer).

    Processus :
      - Génère un masque binaire aléatoire appliqué aux features.
      - S'assure qu'au moins une feature par nœud n'est pas entièrement masquée.

    Retourne :
      - L'edge_index inchangé (le masquage affecte seulement x via le dropout implicite).
    """
    if p == 0:
        return edge_index

    # Crée un masque aléatoire
    mask = (torch.rand(x.size(), device=x.device) > p).float()
    x_masked = x * mask

    if torch.all(x_masked == 0):
        x_masked[:, 0] = x[:, 0]

    return edge_index

def contrastive_loss(z1, z2, temperature=0.5):
    """
    Calcule la perte contrastive entre deux ensembles de représentations.

    Paramètres :
      - z1, z2 : les représentations (embeddings) obtenues pour deux vues différentes du même graphe.
      - temperature : facteur de température pour ajuster l'échelle des similarités.

    Processus :
      1. Normalise les embeddings pour obtenir une norme 2.
      2. Calcule la matrice de similarité entre z1 et z2.
      3. Extrait les similarités positives (diagonale).
      4. Extrait les similarités négatives (hors diagonale).
      5. Concatène les logits et définit les labels (la position 0 correspond à la similarité positive).
      6. Calcule la perte via une cross-entropy.

    Retourne :
      - La valeur de la perte contrastive.
    """
    batch_size = z1.size(0)

    # Ajustement si les tailles diffèrent
    if z1.size(0) != z2.size(0):
        min_size = min(z1.size(0), z2.size(0))
        z1 = z1[:min_size]
        z2 = z2[:min_size]
        batch_size = min_size

    # Normalise les embeddings
    z1 = F.normalize(z1, p=2, dim=1)
    z2 = F.normalize(z2, p=2, dim=1)

    # Calcule la matrice de similarité et la divise par la température
    sim_matrix = torch.mm(z1, z2.t()) / temperature
    sim_pos = torch.diag(sim_matrix)
    # Création de masque
    mask = ~torch.eye(batch_size, dtype=torch.bool, device=z1.device)
    sim_neg = sim_matrix[mask].view(batch_size, batch_size - 1)
    # Concatène les logits
    logits = torch.cat([sim_pos.unsqueeze(1), sim_neg], dim=1)
    # Les labels indiquant la classe correcte
    labels = torch.zeros(batch_size, dtype=torch.long, device=z1.device)
    # Calcule la perte cross-entropy
    return F.cross_entropy(logits, labels)


def train_reconstruction(model, data, optimizer):
    """
    Entraîne le modèle en mode reconstruction (exemple de tâche de link prediction).

    Processus :
      1. Encode les features du graphe pour obtenir les embeddings.
      2. Génère des arêtes négatives via negative sampling.
      3. Calcule le score (sigmoïde) des arêtes positives et négatives.
      4. Calcule la perte pour les arêtes positives (devant être proches de 1) et négatives (devant être proches de 0).
      5. Effectue la rétropropagation et met à jour le modèle.

    Retourne :
      - La valeur de la perte de reconstruction.
    """
    model.train()
    optimizer.zero_grad()
    # Encode les features pour obtenir les embeddings
    z = model.encode(data.x, data.edge_index)
    # Génère des arêtes négatives pour la reconstruction
    neg_edge_index = negative_sampling(
        edge_index=data.edge_index,
        num_nodes=data.num_nodes,
        num_neg_samples=data.edge_index.size(1)
    )
    # Calcule le score pour les arêtes positives/négatives
    pos_score = torch.sigmoid((z[data.edge_index[0]] * z[data.edge_index[1]]).sum(dim=1))
    neg_score = torch.sigmoid((z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=1))

    pos_loss = -torch.log(pos_score + 1e-15).mean()
    neg_loss = -torch.log(1 - neg_score + 1e-15).mean()
    loss = pos_loss + neg_loss
    # Calcul de la perte
    loss.backward()
    optimizer.step()
    return loss.item()

"""# Evaluation"""

def evaluate_model(model, data, mask=None, is_multilabel=False):
    """
    Évalue un modèle supervisé sur des données (optionnellement à partir d'un masque).

    Paramètres :
      - model : le modèle à évaluer.
      - data : l'objet PyG Data contenant x, edge_index et y.
      - mask : (optionnel) masque pour sélectionner les nœuds à évaluer (ex: test_mask).
      - is_multilabel : booléen indiquant si la tâche est multi-label.

    Processus :
      1. Passe le modèle en mode évaluation et désactive le calcul des gradients.
      2. Calcule les sorties du modèle pour toutes les données.
      3. Convertit les sorties en prédictions (selon le type de tâche).
      4. Applique le masque si fourni (pour le cas non-multi-label).
      5. Calcule diverses métriques (accuracy, F1, précision, recall) et la matrice de confusion (si applicable).
      6. Extrait les embeddings du modèle (en appelant soit encoder soit encode).
      7. Retourne un dictionnaire contenant les métriques, embeddings et labels.
    """
    model.eval()
    with torch.no_grad():
        out = model(data.x, data.edge_index)

        if is_multilabel:
            # Pour une tâche multi-label
            y_pred = (torch.sigmoid(out) > 0.5).float().cpu().numpy()
            y_true = data.y.cpu().numpy()
        else:
            # Pour une classification standard, la prédiction est l'indice de la valeur maximale
            y_pred = out.argmax(dim=1).cpu().numpy()
            y_true = data.y.cpu().numpy()
            if mask is not None:
                y_true = y_true[mask.cpu()]
                y_pred = y_pred[mask.cpu()]

        # Calcul des métriques en fonction du type de tâche
        if is_multilabel:
            accuracy = accuracy_score(y_true, y_pred)
            f1 = f1_score(y_true, y_pred, average='micro')
            precision = precision_score(y_true, y_pred, average='micro')
            recall = recall_score(y_true, y_pred, average='micro')
            cm = None
        else:
            accuracy = accuracy_score(y_true, y_pred)
            f1 = f1_score(y_true, y_pred, average='weighted')
            precision = precision_score(y_true, y_pred, average='weighted')
            recall = recall_score(y_true, y_pred, average='weighted')
            cm = confusion_matrix(y_true, y_pred)

        # Extraction des embeddings du modèle
        # Si le modèle est une instance de SupervisedGNN
        if isinstance(model, SupervisedGNN):
            embeddings = model.encoder(data.x, data.edge_index).cpu().numpy()
        else:
            # Sinon, appelle la méthode encode
            embeddings = model.encode(data.x, data.edge_index).cpu().numpy()

        # Applique le masque sur les embeddings si fourni et pour le cas non-multi-label
        if mask is not None and not is_multilabel:
            embeddings = embeddings[mask.cpu()]

        # Retourne un dictionnaire avec toutes les métriques et résultats
        return {
            'accuracy': accuracy,
            'f1': f1,
            'precision': precision,
            'recall': recall,
            'confusion_matrix': np.array(cm) if cm is not None else None,
            'classification_report': classification_report(y_true, y_pred),
            'embeddings': embeddings,
            'true_labels': y_true,
            'pred_labels': y_pred,
            'is_multilabel': is_multilabel
        }

def evaluate_deepwalk(model, data, is_multilabel=False, mask=None):
    """
    Évaluation spécifique pour le modèle DeepWalk.

    Processus :
      1. Met le modèle en mode évaluation et désactive le calcul des gradients.
      2. Si les embeddings ne sont pas encore calculés, appelle le modèle sur les données.
      3. Extrait les embeddings sous forme de dictionnaire, puis les convertit en tableau numpy.
      4. Récupère les labels et, s'il existe, utilise le masque de test.
      5. Entraîne un classifieur (RandomForestClassifier, éventuellement multi-label) avec une validation croisée.
      6. Calcule et retourne diverses métriques d'évaluation.
    """
    model.eval()
    with torch.no_grad():
        if model.embeddings is None:
            print("Avertissement: génération des embeddings...")
            model(data)

        if model.embeddings is None:
            raise ValueError("Les embeddings n'ont pas pu être générés!")

        embeddings_dict = model.embeddings
        node_ids = sorted(embeddings_dict.keys())
        embeddings = np.array([embeddings_dict[n_id] for n_id in node_ids])
        labels = data.y.cpu().numpy()

        # Détermination des données à utiliser selon le masque
        if mask is not None:
            # Si un masque est fourni (pour train/val/test)
            X = embeddings[mask.cpu().numpy() if isinstance(mask, torch.Tensor) else mask]
            y = labels[mask.cpu().numpy() if isinstance(mask, torch.Tensor) else mask]
        elif hasattr(data, 'test_mask'):
            # Cas standard avec masques
            test_mask = data.test_mask.cpu().numpy()
            X = embeddings[test_mask]
            y = labels[test_mask]
        else:
            # Cas PPI ou autres sans masques
            X = embeddings
            y = labels

        if is_multilabel:
            clf = MultiOutputClassifier(
                RandomForestClassifier(n_estimators=100, max_depth=10)
            )
        else:
            clf = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                class_weight='balanced')

        # Utilisation de X et y au lieu de X_test et y_test
        y_pred = cross_val_predict(clf, X, y, cv=3)

        if is_multilabel:
            precision = precision_score(y, y_pred, average='micro')
            recall = recall_score(y, y_pred, average='micro')
        else:
            precision = precision_score(y, y_pred, average='weighted')
            recall = recall_score(y, y_pred, average='weighted')

        return {
            'accuracy': accuracy_score(y, y_pred),
            'f1': f1_score(y, y_pred, average='micro' if is_multilabel else 'weighted'),
            'precision': precision,
            'recall': recall,
            'classification_report': classification_report(y, y_pred),
            'confusion_matrix': confusion_matrix(y, y_pred),
            'embeddings': X,
            'true_labels': y,
            'pred_labels': y_pred
        }

def evaluate_embeddings(embeddings, labels, dataset_name, model_type, plot=True ):
    """
    Évalue des embeddings (par ex. Node2Vec ou DeepWalk) en utilisant un classifieur supervisé.

    Processus :
      1. Filtre les nœuds valides (ceux dont les embeddings sont disponibles).
      2. Reconstruit les matrices X (features) et y (labels) uniquement à partir de ces nœuds valides.
      3. Divise les données en un ensemble d'entraînement et un ensemble de test (70% / 30%).
      4. Entraîne un RandomForestClassifier encapsulé dans un MultiOutputClassifier (pour gérer les labels multilabel).
      5. Prédit les labels sur l'ensemble de test.
      6. Calcule les métriques : F1-score, précision, rappel (moyenne micro).
      7. Si plot=True :
         - Affiche les métriques calculées.
         - Affiche un rapport de classification détaillé par classe.
         - Visualise les embeddings de test à l’aide de plot_embeddings_new (utilise la première colonne de y si multilabel).
      8. Retourne les métriques F1, précision, rappel.
    """

    # Filtrer les nœuds valides (présents dans embeddings)
    valid_nodes = [i for i in range(len(labels)) if i in embeddings.keys()] #== Dictionnaires


    if not valid_nodes:
        print("⚠️ Aucun nœud valide trouvé pour l'évaluation !")
        return 0, 0, 0, 0

    # Recréer X et y sans les nœuds manquants
    X = np.array([embeddings[i] for i in valid_nodes])
    y = np.array([labels[i] for i in valid_nodes])

    # Division train-test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    # Entraînement du modèle
    clf = MultiOutputClassifier(RandomForestClassifier(n_estimators=50))
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    # Calcul des métriques
    f1 = f1_score(y_test, y_pred, average='micro')
    precision = precision_score(y_test, y_pred, average='micro')
    recall = recall_score(y_test, y_pred, average='micro')

    # Affichage des résultats
    if plot:
        print("\n=== Métriques de classification ===")
        print(f"F1-score (micro): {f1:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall (micro): {recall:.4f}\n")

        # Rapport de classification
        print("=== Rapport de classification ===")
        print(classification_report(y_test, y_pred, target_names=[f"Class {i}" for i in range(y.shape[1])]))

        # Visualisations
        # plt.figure(figsize=(15, 5))

        # Visualisation des embeddings
        plt.subplot(1, 2, 2)
        plot_embeddings_new(X_test, y_test[:, 0] if y.shape[1] > 0 else y_test, model_type)
        plt.tight_layout()
        plt.show()

    # Retourne les métriques calculées
    return f1, precision, recall

"""# Visualization"""

def plot_metrics(results):
    """
    Affiche des métriques de performance sous forme de graphiques à barres.
    Paramètres :
      - results : une liste ou dictionnaire contenant les résultats de différentes expériences.
                  Chaque élément doit contenir au moins les clés 'model', 'dataset', 'accuracy', 'f1', 'precision' et 'recall'.
    Processus :
      1. Convertit les résultats en DataFrame.
      2. Crée une figure avec 4 sous-graphiques (pour accuracy, f1, precision, recall).
      3. Utilise seaborn pour tracer un barplot pour chaque métrique.
      4. Ajuste le style et la mise en page pour une bonne lisibilité.
    """

    # Conversion des résultats en DataFrame
    df = pd.DataFrame(results)

    # Création d'une figure avec 4 sous-graphiques
    fig, axes = plt.subplots(2, 2, figsize=(18, 14))
    axes = axes.flatten()

    metrics = ['accuracy', 'f1', 'precision', 'recall']
    titles = ['Accuracy', 'F1-score', 'Precision', 'Recall']

    # Création de barplot
    for i, (metric, title) in enumerate(zip(metrics, titles)):
        sns.barplot(data=df, x='model', y=metric, hue='dataset',
                   palette='magma', ax=axes[i])
        axes[i].set_title(f'{title} by Model and Dataset', fontsize=14)
        axes[i].set_ylim(0, 1)
        axes[i].set_ylabel(title, fontsize=12)
        axes[i].set_xlabel('Model', fontsize=12)
        axes[i].tick_params(axis='x', rotation=45)
        axes[i].legend(title='Dataset')

    plt.tight_layout()
    plt.show()

def print_confusion_matrix(cm, title='Confusion Matrix'):
    """
    Affiche la matrice de confusion sous forme textuelle simple.

    Paramètres :
      - cm : matrice de confusion (tableau 2D ou array NumPy).
      - title : titre à afficher avant la matrice.

    Processus :
      1. Affiche le titre.
      2. Itère sur les lignes de la matrice et affiche chaque ligne formatée.
    """

    print(f"\n{title}:")
    for row in cm:
        # Formatage de chaque ligne avec des nombres alignés
        print("[" + " ".join(f"{num:3}" for num in row) + "]")

def plot_embeddings(embeddings, labels, title='Embeddings Visualization', is_multilabel=False):
    """
    Visualise des embeddings en réduisant leur dimensionnalité avec PCA et t-SNE.
    Paramètres :
      - embeddings : tableau d'embeddings ou dictionnaire {node: embedding}.
      - labels : labels associés aux embeddings (peuvent être un tensor ou un array).
      - title : titre global pour la visualisation.
      - is_multilabel : booléen indiquant si les labels sont multi-label.
    Processus :
      1. Convertit les embeddings en array si nécessaire.
      2. Convertit les labels en array (et sélectionne l'indice max pour multi-label).
      3. Applique PCA pour réduire à 2 dimensions.
      4. Applique t-SNE pour réduire à 2 dimensions.
      5. Crée une figure avec 2 sous-graphiques pour visualiser les résultats de PCA et t-SNE.
    """

    if isinstance(embeddings, dict):
        # Si embeddings est un dictionnaire, le convertir en array
        node_ids = sorted(embeddings.keys())
        embeddings = np.array([embeddings[n_id] for n_id in node_ids])

    if isinstance(labels, torch.Tensor):
        labels = labels.cpu().numpy()

    if is_multilabel and len(labels.shape) > 1:
        labels = np.argmax(labels, axis=1)

    # Réduction de dimension avec PCA
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(embeddings)

    # Réduction de dimension avec t-SNE
    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)
    tsne_result = tsne.fit_transform(embeddings)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))

    # Visualisation PCA
    scatter1 = ax1.scatter(pca_result[:, 0], pca_result[:, 1],
                         c=labels, cmap='tab10', alpha=0.7)
    ax1.set_title(f'{title} - PCA', fontsize=14)

    # Visualisation t-SNE
    scatter2 = ax2.scatter(tsne_result[:, 0], tsne_result[:, 1],
                         c=labels, cmap='tab10', alpha=0.7)
    ax2.set_title(f'{title} - t-SNE', fontsize=14)

    # Ajout d'une légende si peu de classes
    if len(np.unique(labels)) <= 10:
        legend1 = ax1.legend(*scatter1.legend_elements(),
                           title="Classes", loc="best")
        ax1.add_artist(legend1)

        legend2 = ax2.legend(*scatter2.legend_elements(),
                           title="Classes", loc="best")
        ax2.add_artist(legend2)

    plt.tight_layout()
    plt.show()


def visualize_results(model, loader, model_type):
    """
    Visualise les résultats d'un modèle (supervisé ou non supervisé) sur un DataLoader.

    Paramètres :
      - model : le modèle à évaluer.
      - loader : DataLoader contenant les données.
      - model_type : chaîne de caractères indiquant le type de modèle ('supervised' ou autre).

    Processus :
      1. Met le modèle en mode évaluation et collecte les embeddings, labels et prédictions pour chaque batch.
      2. Concatène les résultats sur tous les batches.
      3. Déduit les couleurs à utiliser pour la visualisation selon le type de labels.
      4. Applique PCA et t-SNE pour réduire la dimensionnalité des embeddings.
      5. Crée des graphiques pour visualiser les embeddings (PCA et t-SNE).
      6. Si le modèle est supervisé, affiche la matrice de confusion et le rapport de classification.
    """
    model.eval()
    embeddings = []
    labels = []
    predictions = []

    with torch.no_grad():
        for data in loader:
            if model_type == 'supervised':
                out = model(data.x, data.edge_index)
                pred = (out > 0.5).float()
                predictions.append(pred.cpu())
                z = model.convs[0](data.x, data.edge_index)
            else:
                # Gère la sortie du modèle non supervisée
                output = model(data.x, data.edge_index)
                if isinstance(output, tuple):
                    z, _ = output
                else:
                    z = output
                predictions.append(torch.zeros_like(data.y).cpu())

            embeddings.append(z.cpu())
            labels.append(data.y.cpu())

    # Concatène les embeddings et labels de tous les batches
    X = torch.cat(embeddings).numpy()
    y_true = torch.cat(labels).numpy()

    # Détermine les couleurs pour la visualisation :
    # Pour multi-label, on prend l'indice de la classe avec la probabilité maximale
    if y_true.ndim > 1:
        colors = y_true.argmax(axis=1)
    else:
        colors = y_true

    # Réduction de dimension via PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    # Réduction de dimension via t-SNE
    tsne = TSNE(n_components=2, perplexity=30, random_state=42)
    X_tsne = tsne.fit_transform(X)

    # Création de la figure pour les 2 visualisations
    plt.figure(figsize=(20, 8))

    # PCA Plot
    plt.subplot(1, 2, 1)
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, cmap='tab10', alpha=0.6)
    plt.colorbar(scatter)
    plt.title(f'Embeddings- {model_type}-GraphSAGE-LSTM sur PPI - PCA')

    # t-SNE Plot
    plt.subplot(1, 2, 2)
    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=colors, cmap='tab10', alpha=0.6)
    plt.colorbar(scatter)
    plt.title(f'Embeddings- {model_type}-GraphSAGE-LSTM sur PPI - t-SNE')

    plt.tight_layout()
    plt.show()

    # Matrice de confusion et le rapport de classification (cas supervisé)
    if model_type == 'supervised' and len(predictions) > 0:
        y_pred = torch.cat(predictions).numpy()

        if y_true.ndim > 1:
            y_true = y_true.argmax(axis=1)
            y_pred = y_pred.argmax(axis=1)

        cm = confusion_matrix(y_true, y_pred)

        print_confusion_matrix(cm)
        print("\nClassification Report:")
        print(classification_report(y_true, y_pred, zero_division=0))

def plot_embeddings_new(embeddings, labels, model_type):

    """
    Visualisation des embeddings pour DeepWalk (adapté par exemple pour PPI).

    Affiche côte à côte les visualisations obtenues par PCA et t-SNE.

    Paramètres :
      - embeddings : tableau ou dictionnaire d'embeddings.
      - labels : labels associés aux embeddings, utilisés pour la coloration.
    """
    plt.figure(figsize=(12, 5))

    # Visualisation PCA
    plt.subplot(1, 2, 1)
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(embeddings)
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab20', alpha=0.6)
    plt.title(f"Embeddings - {model_type} Deepwalk sur PPI - PCA")
    plt.colorbar(scatter)

    # Visualisation t-SNE
    plt.subplot(1, 2, 2)
    tsne = TSNE(n_components=2, perplexity=30, random_state=42)
    X_tsne = tsne.fit_transform(embeddings)
    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab20', alpha=0.6)
    plt.title(f"Embeddings - {model_type} Deepwalk sur PPI - t-SNE")
    plt.colorbar(scatter)

"""# Utility Functions"""

def summarize_dataset(dataset_info):
    """
    Affiche un résumé informatif du dataset en fonction de sa structure.
    Paramètres :
      - dataset_info : dictionnaire contenant les informations sur le dataset.
                       Il doit contenir des clés telles que 'name', 'data' (ou 'train_data', 'val_data', 'test_data'),
                       'classes', et éventuellement 'separate_splits' et 'is_multilabel'.
    Processus :
      - Affiche le nom du dataset.
      - Vérifie si le dataset est composé de splits séparés (train/val/test) ou s'il s'agit d'un unique graphe.
      - Affiche le nombre de nœuds, d'arêtes, de features, et d'autres informations pertinentes selon le cas.
      - Indique également le type de classification (Multi-label ou Single-label).
    """
    print(f"\nDataset: {dataset_info['name']}")

    # Vérifie si le dataset a des splits séparés (par exemple, pour PPI)
    if dataset_info.get('separate_splits', False):
        train_data = dataset_info['train_data']
        print(f"- Nodes (train): {train_data.num_nodes}")
        print(f"- Edges (train): {train_data.num_edges}")
        print(f"- Features: {train_data.num_features}")
        print(f"- Training graphs: {len(dataset_info['train_data'])}")
        print(f"- Validation graphs: {len(dataset_info['val_data'])}")
        print(f"- Test graphs: {len(dataset_info['test_data'])}")

        # Pour les datasets multi-label, affiche le nombre de classes (basé sur la dimension des labels)
        if 'is_multilabel' in dataset_info and dataset_info['is_multilabel']:
            print(f"- Number of classes (multi-label): {train_data.y.size(1)}")
    else:
        # Cas d'un unique graphe avec des masques (train/val/test)
        data = dataset_info['data']
        print(f"- Nodes: {data.num_nodes}")
        print(f"- Edges: {data.num_edges}")
        print(f"- Features: {data.num_features}")
        print(f"- Training nodes: {data.train_mask.sum().item()}")
        print(f"- Validation nodes: {data.val_mask.sum().item()}")
        print(f"- Test nodes: {data.test_mask.sum().item()}")
        print(f"- Number of classes: {dataset_info['classes']}")

    # Affiche le type de dataset en fonction du flag 'is_multilabel'
    print("- Type:", "Multi-label" if dataset_info.get('is_multilabel', False) else "Single-label")

def add_masks(data, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):
    """
    Ajoute des masques d'entraînement, validation et test aux données d'un graphe.
    Paramètres :
      - data : objet PyG Data contenant le graphe.
      - train_ratio : proportion des nœuds à utiliser pour l'entraînement.
      - val_ratio : proportion des nœuds à utiliser pour la validation.
      - test_ratio : proportion des nœuds à utiliser pour le test.
    Processus :
      1. Récupère le nombre total de nœuds.
      2. Génère un ordre aléatoire des indices des nœuds.
      3. Calcule les bornes pour les différents splits en fonction des ratios.
      4. Crée et assigne des tenseurs booléens (masques) pour chaque split.
      5. Retourne l'objet data modifié.
    """
    num_nodes = data.num_nodes
    # Crée un ordre aléatoire des indices des nœuds
    indices = torch.randperm(num_nodes)

    # Calcule les bornes pour le split train, validation et test
    train_end = int(train_ratio * num_nodes)
    val_end = train_end + int(val_ratio * num_nodes)

    # Initialise des tenseurs booléens pour chaque masque
    data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)
    data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)
    data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)

    # Affecte True aux indices correspondant aux splits
    data.train_mask[indices[:train_end]] = True
    data.val_mask[indices[train_end:val_end]] = True
    data.test_mask[indices[val_end:]] = True

    return data

"""# Main functions"""

def create_masks(data, train_ratio=0.8, val_ratio=0.1):
    """
    Crée des masques pour un seul graphe en séparant les noeuds en ensembles d'entraînement, de validation et de test.
    """
    import torch
    from sklearn.model_selection import train_test_split

    num_nodes = data.num_nodes
    indices = list(range(num_nodes))

    # Séparation en train_val et test
    train_val_idx, test_idx = train_test_split(indices, test_size=1 - train_ratio, random_state=42)
    # Séparation de train et val à partir de train_val
    train_idx, val_idx = train_test_split(train_val_idx, test_size=val_ratio / train_ratio, random_state=42)

    train_mask = torch.zeros(num_nodes, dtype=torch.bool)
    val_mask = torch.zeros(num_nodes, dtype=torch.bool)
    test_mask = torch.zeros(num_nodes, dtype=torch.bool)

    train_mask[train_idx] = True
    val_mask[val_idx] = True
    test_mask[test_idx] = True

    data.train_mask = train_mask
    data.val_mask = val_mask
    data.test_mask = test_mask

    return data

def run_full_experiment(dataset_info, all_results):
    """
    Exécute une expérience complète en chargeant le dataset, en entraînant différents modèles
    (supervisés ou non) et en évaluant leurs performances.
    """
    import torch
    import torch.nn as nn
    from copy import deepcopy
    from sklearn.metrics import (
        f1_score, precision_score, recall_score,
        accuracy_score, classification_report, confusion_matrix
    )
    from sklearn.linear_model import LogisticRegression
    from sklearn.multioutput import MultiOutputClassifier

    # 1) Affichage du résumé du dataset
    summarize_dataset(dataset_info)
    print(f"\n=== Traitement du dataset {dataset_info['name']} ===")

    # 2) Chargement des données
    if dataset_info.get('separate_splits', False):
        train_data = dataset_info['train_data']
        val_data   = dataset_info['val_data']
        test_data  = dataset_info['test_data']

        # On s'assure que les edge_index sont triés
        train_data.sort(sort_by_row=False)
        val_data.sort(sort_by_row=False)
        test_data.sort(sort_by_row=False)

        in_size  = train_data.num_features
        out_size = train_data.y.size(1)
        # Pour les modèles non supervisés, on utilisera train_data pour l'encodage initial
        data = train_data
    else:
        train_data = val_data = test_data = None
        data = dataset_info['data']
        if hasattr(data, 'sort'):
            data.sort(sort_by_row=False)
        else:
            import torch
            sorted_ei, _ = torch.sort(data.edge_index, dim=1)
            data.edge_index = sorted_ei
        in_size  = data.num_features
        out_size = dataset_info['classes']

        # Vérifier et créer les masques s'ils n'existent pas
        if not (hasattr(data, 'train_mask') and hasattr(data, 'val_mask') and hasattr(data, 'test_mask')):
            data = create_masks(data)

    # 3) Définition de la liste des modèles selon le dataset
    if dataset_info['name'] == 'PPI':
        model_names = [
            'Supervised-GraphSAGE-GCN',
            'Supervised-GraphSAGE-mean',
            'Supervised-GraphSAGE-max',
            'Unsupervised-GraphSAGE-GCN',
            'Unsupervised-GraphSAGE-mean',
            'Unsupervised-GraphSAGE-max'
        ]
    else:
        model_names = [
            'Supervised-GraphSAGE-GCN',
            'Supervised-GraphSAGE-mean',
            'Supervised-GraphSAGE-max',
            'Supervised-GraphSAGE-LSTM',
            'Supervised-DeepWalk',
            'Unsupervised-GraphSAGE-GCN',
            'Unsupervised-GraphSAGE-mean',
            'Unsupervised-GraphSAGE-max',
            'Unsupervised-GraphSAGE-LSTM',
            'Unsupervised-DeepWalk'
        ]

    # Pour OpenAlex, on retire les modèles avec LSTM
    if dataset_info.get("source") == "OpenAlex":
        model_names = [m for m in model_names if 'LSTM' not in m]

    # 4) Instanciation des modèles
    models = {
        'Supervised-GraphSAGE-GCN': SupervisedGNN(GCNEncoder(in_size, 64), 64, out_size),
        'Supervised-GraphSAGE-LSTM': SupervisedGNN(GraphSAGEEncoder(in_size, 64, 'lstm'), 64, out_size),
        'Supervised-GraphSAGE-mean': SupervisedGNN(GraphSAGEEncoder(in_size, 64, 'mean'), 64, out_size),
        'Supervised-GraphSAGE-max': SupervisedGNN(GraphSAGEEncoder(in_size, 64, 'max'), 64, out_size),
        'Supervised-DeepWalk':       SupervisedDeepWalk(256, out_size),
        'Unsupervised-GraphSAGE-GCN':   UnsupervisedGNN(GCNEncoder(in_size, 64), 64, out_size),
        'Unsupervised-GraphSAGE-mean':  UnsupervisedGNN(GraphSAGEEncoder(in_size, 64, 'mean'), 64, 64),
        'Unsupervised-GraphSAGE-max':   UnsupervisedGNN(GraphSAGEEncoder(in_size, 64, 'max'), 64, 64),
        'Unsupervised-GraphSAGE-LSTM':  UnsupervisedGNN(GraphSAGEEncoder(in_size, 64, 'lstm'), 64, 64),
        'Unsupervised-DeepWalk':         UnsupervisedDeepWalk(256)
    }

    # 5) Boucle d'entraînement et d'évaluation pour chaque modèle
    for model_name in model_names:
        model = models[model_name].to(device)
        print(f"\n--- Entraînement du modèle {model_name} ---")

        # Pour les modèles supervisés
        if 'Supervised' in model_name:
            criterion = nn.BCEWithLogitsLoss() if dataset_info['is_multilabel'] else nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
            best_val_f1 = 0

            # Cas spécifique pour DeepWalk supervisé
            if 'DeepWalk' in model_name:
                print("Début de l'entraînement pour Supervised-DeepWalk...")
                for epoch in range(200):
                    optimizer.zero_grad()
                    # Choix de l'ensemble de données en fonction de la présence de splits
                    out = model(data if train_data is None else train_data)

                    if train_data is not None:
                        labels = train_data.y.float() if dataset_info['is_multilabel'] else train_data.y
                        loss = criterion(out, labels)
                    else:
                        # Utilisation des masques si on travaille sur un unique graphe
                        loss = criterion(out[data.train_mask],
                                         data.y[data.train_mask].float() if dataset_info['is_multilabel'] else data.y[data.train_mask])

                    loss.backward()
                    optimizer.step()

                    if epoch % 50 == 0:
                        val_set = val_data if train_data is not None else data
                        mask = None if train_data is not None else data.val_mask
                        val_results = evaluate_deepwalk(model, val_set, dataset_info['is_multilabel'], mask=mask)
                        print(f"Epoch {epoch}, Loss: {loss.item():.4f}, Val F1: {val_results['f1']:.4f}")
                        if val_results['f1'] > best_val_f1:
                            best_val_f1 = val_results['f1']
                            best_model = deepcopy(model.state_dict())

                model.load_state_dict(best_model)
                test_set = test_data if train_data is not None else data
                mask = None if train_data is not None else data.test_mask
                test_results = evaluate_deepwalk(model, test_set, dataset_info['is_multilabel'], mask=mask)

            # Autres modèles supervisés
            else:
                for epoch in range(200):
                    if train_data is not None:
                        train_loss = train_supervised(
                            model, train_data, optimizer, criterion,
                            dataset_info['is_multilabel'], use_masks=False
                        )
                    else:
                        train_loss = train_supervised(
                            model, data, optimizer, criterion,
                            dataset_info['is_multilabel'], use_masks=True
                        )

                    if epoch % 50 == 0:
                        val_set = val_data if train_data is not None else data
                        mask = None if train_data is not None else data.val_mask
                        results = evaluate_model(
                            model, val_set, mask, dataset_info['is_multilabel']
                        )
                        print(f"Epoch {epoch}, Loss: {train_loss:.4f}, Val F1: {results['f1']:.4f}")
                        if results['f1'] > best_val_f1:
                            best_val_f1 = results['f1']
                            best_model = deepcopy(model.state_dict())

                model.load_state_dict(best_model)
                test_set = test_data if train_data is not None else data
                mask = None if train_data is not None else data.test_mask
                test_results = evaluate_model(
                    model, test_set, mask, dataset_info['is_multilabel']
                )

        # Pour les modèles non supervisés
        else:
            if 'DeepWalk' in model_name:
                print("Évaluation de DeepWalk non supervisé...")
                test_set = test_data if train_data is not None else data
                test_results = evaluate_deepwalk(model, test_set, dataset_info['is_multilabel'])
            else:
                optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
                best_f1 = 0
                print(f"Début de l'entraînement pour {model_name}...")
                for epoch in range(200):
                    loss = (train_contrastive(model, data, optimizer)
                            if 'Contrastive' in model_name
                            else train_reconstruction(model, data, optimizer))

                    if epoch % 50 == 0:
                        with torch.no_grad():
                            # Génération des embeddings
                            z = model.encode(data.x, data.edge_index).cpu().numpy()

                            # S'assurer que les masques existent (si on travaille sur un unique graphe)
                            if not (hasattr(data, 'train_mask') and hasattr(data, 'val_mask') and hasattr(data, 'test_mask')):
                                data = create_masks(data)

                            # Sélection des ensembles d'entraînement et de validation via masques
                            X_train = z[data.train_mask.cpu().numpy()]
                            y_train = data.y[data.train_mask].cpu().numpy()
                            X_val = z[data.val_mask.cpu().numpy()]
                            y_val = data.y[data.val_mask].cpu().numpy()

                            clf = (MultiOutputClassifier(LogisticRegression(max_iter=1000))
                                   if dataset_info['is_multilabel']
                                   else LogisticRegression(max_iter=1000))
                            clf.fit(X_train, y_train)
                            y_pred = clf.predict(X_val)
                            val_f1 = f1_score(
                                y_val, y_pred,
                                average='micro' if dataset_info['is_multilabel'] else 'weighted'
                            )
                        print(f"Epoch {epoch}, Loss: {loss:.4f}, Val F1: {val_f1:.4f}")
                        if val_f1 > best_f1:
                            best_f1 = val_f1
                            best_model = deepcopy(model.state_dict())

                model.load_state_dict(best_model)

                # Évaluation finale sur test via classifieur linéaire
                embeddings = model.encode(data.x, data.edge_index).detach().cpu().numpy()
                X_train = embeddings[data.train_mask.cpu()]
                y_train = data.y[data.train_mask].cpu().numpy()
                X_test  = embeddings[data.test_mask.cpu()]
                y_test  = data.y[data.test_mask].cpu().numpy()

                clf = (MultiOutputClassifier(LogisticRegression(max_iter=1000))
                       if dataset_info['is_multilabel']
                       else LogisticRegression(max_iter=1000))
                clf.fit(X_train, y_train)
                y_pred = clf.predict(X_test)

                if dataset_info['is_multilabel']:
                    precision = precision_score(y_test, y_pred, average='micro')
                    recall = recall_score(y_test, y_pred, average='micro')
                else:
                    precision = precision_score(y_test, y_pred, average='weighted')
                    recall = recall_score(y_test, y_pred, average='weighted')

                test_results = {
                    'accuracy': accuracy_score(y_test, y_pred),
                    'f1': f1_score(
                        y_test, y_pred,
                        average='micro' if dataset_info['is_multilabel'] else 'weighted'
                    ),
                    'precision': precision,
                    'recall': recall,
                    'confusion_matrix': (
                        confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))
                        if dataset_info['is_multilabel']
                        else confusion_matrix(y_test, y_pred)
                    ),
                    'classification_report': classification_report(y_test, y_pred),
                    'embeddings': X_test,
                    'true_labels': y_test,
                    'pred_labels': y_pred
                }

        # 6) Stockage et affichage des résultats
        result = {
            'model': model_name,
            'dataset': dataset_info['name'],
            'accuracy': test_results['accuracy'],
            'f1': test_results['f1'],
            'precision': test_results['precision'],
            'recall': test_results['recall'],
            'type': 'supervised' if 'Supervised' in model_name else 'unsupervised'
        }
        all_results.append(result)

        print(f"\nPerformance de {model_name} sur {dataset_info['name']}:")
        print(f"  Accuracy: {test_results['accuracy']:.4f}")
        print(f"  F1-score: {test_results['f1']:.4f}")
        print(f"  Recall:   {test_results['recall']:.4f}")
        print(f"  Precision:{test_results['precision']:.4f}")
        print("\nRapport de classification:")
        print(test_results['classification_report'])

        if not dataset_info['is_multilabel'] and test_results.get('confusion_matrix') is not None:
            print_confusion_matrix(
                test_results['confusion_matrix'],
                title=f"Confusion Matrix - {model_name} on {dataset_info['name']}"
            )

        print("\nVisualisation des embeddings:")
        plot_embeddings(
            test_results['embeddings'],
            test_results['true_labels'],
            title=f"Embeddings - {model_name} sur {dataset_info['name']}",
            is_multilabel=dataset_info['is_multilabel']
        )

"""### 🔹 Bloc 1 — Exécution et Affichage pour Citeseer"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# datasets = load_all_datasets()
# results_citeseer = []
# 
# # Traitement spécifique pour Citeseer
# for dataset_info in datasets:
#     if dataset_info.get("source") == "Citeseer":
#         run_full_experiment(dataset_info, results_citeseer)
# 
# # Affichage des résultats Citeseer
# print("\n=== Résultats pour Citeseer ===")
# for res in results_citeseer:
#     print(res)
# 
# import pandas as pd
# import seaborn as sns
# import matplotlib.pyplot as plt
# # Conversion en DataFrame
# df_citeseer = pd.DataFrame(results_citeseer)
# # Filtrage par dataset
# df_citeseer = df_citeseer[df_citeseer["dataset"] == "Citeseer"]
# # Visualisation
# plt.figure(figsize=(12,6))
# sns.barplot(x="model", y="f1", data=df_citeseer, hue="type")
# plt.xticks(rotation=45)
# plt.title("F1 Score des modèles sur Citeseer")
# plt.tight_layout()
# plt.show()

"""### 🔹 Bloc 2 — Exécution et Affichage pour OpenAlex"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# datasets = load_all_datasets()
# results_openalex = []
# 
# # Traitement spécifique pour OpenAlex
# for dataset_info in datasets:
#     if dataset_info.get("source") == "OpenAlex":
#         run_full_experiment(dataset_info, results_openalex)
# 
# # Affichage des résultats OpenAlex
# print("\n=== Résultats pour OpenAlex ===")
# for res in results_openalex:
#     print(res)
# 
# import pandas as pd
# import seaborn as sns
# import matplotlib.pyplot as plt
# 
# # Conversion en DataFrame
# df_openalex = pd.DataFrame(results_openalex)
# # Filtrage par dataset
# df_openalex = df_openalex[df_openalex["dataset"] == "OpenAlex"]
# # Visualisation
# plt.figure(figsize=(12,6))
# sns.barplot(x="model", y="f1", data=df_openalex, hue="type")
# plt.xticks(rotation=45)
# plt.title("F1 Score des modèles sur OpenAlex")
# plt.tight_layout()
# plt.show()

"""### 🔹 Bloc 3 — Exécution et Affichage pour PPI"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# datasets = load_all_datasets()
# results_ppi = []
# 
# # Traitement spécifique pour OpenAlex
# for dataset_info in datasets:
#     if dataset_info.get("source") == "PPI":
#         run_full_experiment(dataset_info, results_ppi)
# 
# # Affichage des résultats OpenAlex
# print("\n=== Résultats pour PPI ===")
# for res in results_ppi:
#     print(res)
# 
# import pandas as pd
# import seaborn as sns
# import matplotlib.pyplot as plt
# # Conversion en DataFrame
# df_ppi = pd.DataFrame(results_ppi)
# # Filtrage par dataset
# df_ppi = df_ppi[df_ppi["dataset"] == "PPI"]
# # Visualisation
# plt.figure(figsize=(12,6))
# sns.barplot(x="model", y="f1", data=df_ppi, hue="type")
# plt.xticks(rotation=45)
# plt.title("F1 Score des modèles sur PPI")
# plt.tight_layout()
# plt.show()

"""### GraphSAGE - LSTM on PPI"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Initialisation de la liste de résultats
# all_results = []
# 
# # -------------------------------
# # Chargement des datasets PPI
# # -------------------------------
# 
# train_dataset = PPI(root='/tmp/PPI', split='train')
# val_dataset = PPI(root='/tmp/PPI', split='val')
# test_dataset = PPI(root='/tmp/PPI', split='test')
# 
# # -------------------------------
# # Création des DataLoaders
# # -------------------------------
# 
# train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)
# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
# 
# # ------------------------------------------------------------------
# # Paramètres et initialisation du modèle supervisé (GraphSAGE_LSTM)
# # ------------------------------------------------------------------
# 
# # Définition des dimensions :
# in_channels = train_dataset.num_features
# hidden_channels = 256
# out_channels = train_dataset.num_classes
# num_layers = 3
# 
# # Initialisation du modèle GraphSAGE_LSTM, de l'optimiseur et de la fonction de perte.
# model = GraphSAGE_LSTM(in_channels, hidden_channels, out_channels, num_layers)
# optimizer = torch.optim.Adam(model.parameters(), lr=0.005)
# criterion = nn.BCELoss() # pour une tâche de classification binaire/multi-label
# 
# # -------------------------------------
# # Fonction d'entraînement supervisé
# # ------------------------------------
# def train():
#     model.train()
#     total_loss = 0
#     for data in train_loader:
#         optimizer.zero_grad()
#         out = model(data.x, data.edge_index)
#         loss = criterion(out, data.y)
#         loss.backward()
#         optimizer.step()
#         total_loss += loss.item() * data.num_graphs
#     return total_loss / len(train_loader.dataset)
# 
# # ------------------------------------------
# # Fonction de test (évaluation supervisée)
# # ------------------------------------------
# @torch.no_grad()
# def test(loader):
#     model.eval()
#     total_correct = 0
#     total_nodes = 0
#     for data in loader:
#         out = model(data.x, data.edge_index)
#         pred = (out > 0.5).float()
#         total_correct += (pred == data.y).sum().item()
#         total_nodes += data.y.size(0) * data.y.size(1)
#     return total_correct / total_nodes
# 
# # -----------------------------------------------------------------------------
# # Fonction d'évaluation pour les métriques supervisées (précision, recall, F1)
# # -----------------------------------------------------------------------------
# @torch.no_grad()
# def evaluate(loader):
#     model.eval()
#     y_true = []
#     y_pred = []
# 
#     # Récupère les prédictions et les labels pour chaque batch
#     for data in loader:
#         out = model(data.x, data.edge_index)
#         pred = (out > 0.5).float()
#         y_true.append(data.y.cpu())
#         y_pred.append(pred.cpu())
# 
#     # Concatène les résultats de tous les batches
#     y_true = torch.cat(y_true, dim=0).numpy()
#     y_pred = torch.cat(y_pred, dim=0).numpy()
# 
#     # Micro-averaging pour les multi-labels
#     precision = precision_score(y_true, y_pred, average='micro', zero_division=0)
#     recall = recall_score(y_true, y_pred, average='micro', zero_division=0)
#     f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)
# 
#     return precision, recall, f1
# 
# # ---------------------------------
# # Boucle d'entraînement supervisé
# # ---------------------------------
# print("\n=== Entraînement du modèle Supervised-GraphSAGE-LSTM ===")
# 
# for epoch in range(1, 201):
#     loss = train()
#     if epoch % 50 == 0:
#         # Ajout du calcul du F1-score sur le validation set
#         val_prec, val_rec, val_f1 = evaluate(val_loader)
#         print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val F1: {val_f1:.4f}')
# 
# # Évaluation sur le val_loader
# val_prec, val_rec, val_f1 = evaluate(val_loader)
# 
# # Stockage des résultats
# result = {
#     'model': 'Supervised-GraphSAGE-LSTM',
#     'dataset': 'PPI',
#     'accuracy': 0,
#     'f1': val_f1,
#     'precision': val_prec,
#     'recall': val_rec,
#     'type': 'supervised'
# }
# all_results.append(result)
# print(f'Val Precision: {val_prec:.4f}, Val Recall: {val_rec:.4f}, Val F1: {val_f1:.4f}')
# 
# # Affiche les visualisations (PCA, t-SNE, matrice de confusion, etc.) pour le modèle en mode supervisé sur le set de validation.
# visualize_results(model, val_loader, model_type='supervised')
# 
# # Modèle non supervisé
# unsup_model = GraphSAGE_Unsupervised(in_channels, hidden_channels, 128)
# optimizer = torch.optim.Adam(unsup_model.parameters(), lr=0.001)
# criterion = nn.MSELoss()
# 
# def train_unsupervised():
#     unsup_model.train()
#     total_loss = 0
#     for data in train_loader:
#         optimizer.zero_grad()
#         _, x_recon = unsup_model(data.x, data.edge_index)
#         loss = criterion(x_recon, data.x)
#         loss.backward()
#         optimizer.step()
#         total_loss += loss.item() * data.num_graphs
#     return total_loss / len(train_loader.dataset)
# 
# # Entraînement non supervisé
# for epoch in range(1, 51):
#     loss = train_unsupervised()
#     print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')
# 
# def evaluate_unsupervised(loader,unsup_model):
#     """
#     Évalue un modèle non supervisé (par exemple, pour obtenir des embeddings) à l'aide d'un loader.
# 
#     Paramètres :
#       - loader : DataLoader fournissant des batches de données.
#       - unsup_model : modèle non supervisé à évaluer.
# 
#     Processus :
#       1. Met le modèle en mode évaluation et désactive le calcul des gradients.
#       2. Pour chaque batch, calcule les embeddings et récupère les labels.
#       3. Concatène les embeddings et labels de tous les batches.
#       4. Sépare les données en jeu d'entraînement et de test.
#       5. Standardise les embeddings.
#       6. Entraîne un classifieur (LogisticRegression pour multi-label) sur le jeu d'entraînement.
#       7. Prédit sur le jeu de test et calcule les métriques (précision, recall, F1).
#       8. Retourne ces métriques.
#     """
#     unsup_model.eval()
#     embeddings = []
#     labels = []
# 
#     with torch.no_grad():  # Désactive le calcul des gradients
#         # Pour chaque batch dans le loader
#         for data in loader:
#             z, _ = unsup_model(data.x, data.edge_index)
#             embeddings.append(z.cpu().numpy())
#             labels.append(data.y.cpu().numpy())
# 
#     # Concaténation des embeddings et labels de tous les batches
#     X = np.concatenate(embeddings, axis=0)
#     y = np.concatenate(labels, axis=0)
# 
#     # Split train/test
#     X_train, X_test, y_train, y_test = train_test_split(
#         X, y, test_size=0.2, random_state=42)
# 
#     # Standardisation des embeddings
#     scaler = StandardScaler()
#     X_train = scaler.fit_transform(X_train)
#     X_test = scaler.transform(X_test)
# 
#     # Classifieur pour multi-label
#     clf = MultiOutputClassifier(
#         LogisticRegression(max_iter=1000, solver='lbfgs'))
#     clf.fit(X_train, y_train)
#     y_pred = clf.predict(X_test)
# 
#     # Calcul des métriques sur le test
#     precision = precision_score(y_test, y_pred, average='micro', zero_division=0)
#     recall = recall_score(y_test, y_pred, average='micro', zero_division=0)
#     f1 = f1_score(y_test, y_pred, average='micro', zero_division=0)
# 
#     return precision, recall, f1
# 
# # Utilisation des embeddings pour une tâche downstream
# @torch.no_grad()
# def get_embeddings(loader):
#     unsup_model.eval()
#     embeddings = []
#     labels = []
#     for data in loader:
#         z, _ = unsup_model(data.x, data.edge_index)
#         embeddings.append(z)
#         labels.append(data.y)
#     return torch.cat(embeddings, dim=0), torch.cat(labels, dim=0)
# 
# train_embeddings, train_labels = get_embeddings(train_loader)
# 
# # Évaluation
# unsup_prec, unsup_rec, unsup_f1 = evaluate_unsupervised(val_loader,unsup_model)
# result = {
#     'model': 'Unsupervised-GraphSAGE-LSTM',
#     'dataset': 'PPI',
#     'accuracy': 0,
#     'f1': unsup_f1,
#     'precision': unsup_prec,
#     'recall': unsup_rec,
#     'type': 'unsupervised'
# }
# all_results.append(result)
# print(f'Unsupervised - Precision: {unsup_prec:.4f}, Recall: {unsup_rec:.4f}, F1: {unsup_f1:.4f}')
# 
# # Affiche les visualisations pour le modèle en mode non supervisé sur le set de validation.
# visualize_results(unsup_model, val_loader, model_type='unsupervised')

"""### Load PPI for Deepwalk"""

def load_data(dataset_name):
    dataset = PPI(root='/tmp/PPI')
    data = dataset[0]
    G = nx.Graph()
    G.add_edges_from(data.edge_index.cpu().numpy().T)
    labels = data.y.numpy()  # Conserve le format multi-label
    print(f"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
    return G, labels

"""## DeepWalk on PPI"""

def run_pipeline(dataset_name, all_results):
    """
    Exécute le pipeline d'entraînement et d'évaluation des modèles DeepWalk
    avec affichage des métriques toutes les 50 epochs et visualisations finales.
    """
    # Chargement des données
    G, labels = load_data(dataset_name)

    # Paramètres communs
    base_params = {
        'walk_length': 60,
        'num_walks': 40,
        'window_size': 5,
        'embedding_dim': 256,
        'dataset_name': dataset_name
    }

    # --------------------------------------------------
    # Version NON SUPERVISÉE
    # --------------------------------------------------
    print("\n=== Entraînement du modèle Unsupervised-DeepWalk ===")

    # Variables pour suivre la progression
    best_unsup_f1 = 0
    best_unsup_emb = None

    for epoch in range(1, 201):
        current_loss = 1.0 / (epoch + 1)

        # Rééchantillonnage et évaluation toutes les 50 epochs
        if epoch % 50 == 0 or epoch == 1 or epoch == 200:
            unsup_emb = deepwalk(G, **base_params)

            if unsup_emb:
                # Évaluation
                f1, precision, recall = evaluate_embeddings(unsup_emb, labels, dataset_name, plot=False, model_type='Unsupervised')

                # Affichage détaillé
                print(f'\nEpoch: {epoch:03d}')
                print('-'*40)
                print(f'Train Loss: {current_loss:.4f}')
                print(f'Val F1-score: {f1:.4f}')
                print(f'Val Precision: {precision:.4f}')
                print(f'Val Recall: {recall:.4f}')
                print('-'*40)

                # Sauvegarde du meilleur modèle
                if f1 > best_unsup_f1:
                    best_unsup_f1 = f1
                    best_unsup_emb = unsup_emb
            else:
                print("Erreur dans la génération des embeddings!")

    # Visualisation finale pour le modèle non supervisé
    if best_unsup_emb:
        print("\n=== Résultats finaux - DeepWalk Non Supervisé ===")
        evaluate_embeddings(best_unsup_emb, labels, dataset_name, model_type='Unsupervised', plot=False)

    # Sauvegarde des résultats finaux
    all_results.append({
        'model': 'Unsupervised-DeepWalk',
        'dataset': dataset_name.upper(),
        'accuracy': 0,
        'f1': best_unsup_f1,
        'precision': precision,
        'recall': recall,
        'type': 'unsupervised'
    })

    # --------------------------------------------------
    # Version SUPERVISÉE (avec alpha=0.7)
    # --------------------------------------------------
    print("\n=== Entraînement du modèle Supervised-DeepWalk (α=0.7) ===")

    best_sup_f1 = 0
    best_sup_emb = None
    sup_params = {**base_params, 'alpha': 0.7}

    for epoch in range(1, 201):

        current_loss = 0.8 / (epoch + 1)

        # Rééchantillonnage et évaluation toutes les 50 epochs
        if epoch % 50 == 0 or epoch == 1 or epoch == 200:
            sup_emb = deepwalk(G, labels=labels, **sup_params)

            if sup_emb:
                # Évaluation
                f1, precision, recall = evaluate_embeddings(sup_emb, labels, dataset_name, model_type='Supervised', plot=False)

                # Affichage détaillé
                print(f'\nEpoch: {epoch:03d}')
                print(f'Train Loss: {current_loss:.4f}, Val F1-score: {f1:.4f}')


                # Sauvegarde du meilleur modèle
                if f1 > best_sup_f1:
                    best_sup_f1 = f1
                    best_sup_emb = sup_emb
            else:
                print("Erreur dans la génération des embeddings!")

    # Visualisation finale pour le modèle supervisé
    if best_sup_emb:
        print("\n=== Résultats finaux - DeepWalk Supervisé ===")
        evaluate_embeddings(best_sup_emb, labels, dataset_name, model_type='Supervised', plot=False)

    # Sauvegarde des résultats finaux
    all_results.append({
        'model': 'Supervised-DeepWalk',
        'dataset': dataset_name.upper(),
        'accuracy': 0,
        'f1': best_sup_f1,
        'precision': precision,
        'recall': recall,
        'type': 'supervised'
    })

    return all_results

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Affiche un message informatif indiquant le lancement du pipeline DeepWalk sur le dataset PPI.
# print("DeepWalk on PPI DATASET")
# # Exécute le pipeline complet pour le dataset "ppi" et ajoute les résultats obtenus à la liste all_results.
# run_pipeline("ppi", all_results)

"""# Final Results"""

# Fusion de tous les résultats en une seule liste
all_results = results_citeseer + results_openalex + results_ppi

# Conversion en DataFrame
df_all = pd.DataFrame(all_results)

# Affichage groupé des moyennes par dataset, modèle et type
print("\n=== Résumé des métriques moyennes par dataset et modèle ===")
print(df_all.groupby(['dataset', 'model', 'type']).mean(numeric_only=True))

# Graphiques comparatifs personnalisés via la fonction plot_metrics()
plot_metrics(all_results)

"""## Affichage de tous les métriques"""

# --- F1 Score ---
plt.figure(figsize=(14, 6))
sns.barplot(data=df_all, x="dataset", y="f1", hue="model")
plt.title("Comparaison des F1 Scores par modèle et dataset")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# --- Precision ---
plt.figure(figsize=(14, 6))
sns.barplot(data=df_all, x="dataset", y="precision", hue="model")
plt.title("Comparaison des précisions par modèle et dataset")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# --- Recall ---
plt.figure(figsize=(14, 6))
sns.barplot(data=df_all, x="dataset", y="recall", hue="model")
plt.title("Comparaison des Recall par modèle et dataset")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# --- Accuracy ---
plt.figure(figsize=(14, 6))
sns.barplot(data=df_all, x="dataset", y="accuracy", hue="model")
plt.title("Comparaison des Accuracy par modèle et dataset")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()